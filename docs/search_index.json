[["index.html", "R Programming for Statistical Analysis Chapter 1 Welcome", " R Programming for Statistical Analysis Hammed Akande, Jeff Sauer, Abimbola Ogungbire, Tomiyosi Bola 2021-07-02 Chapter 1 Welcome Thanks for your interest in R Programming for statistical analysis. This is a three-day workshop from June 28 to June 30 (4 PM GMT, 11 AM EDT, 8 AM PDT on all days). This workshop aims to expose early-career scientists to coding and basic statistical analysis in R while fostering collaborations and knowledge sharing among participants. Specifically, we hope this workshop will enable young scholars to build expertise in R and make them competitive in global scholarships. This book is intended to serve as a guide to learning R. Good thing is that the book is (and will be) FREE. So, whether you are attending the workshop live or not, you can always consult this book later on for reference. Please feel free to contribute to this open source project if you would want to add or correct the book. You can submit a pull request on Github or send a the corrections (for example, in R Markdown) and we update the book. As this is only for 3days and limited with time, this workshop will only introduce you to basic coding and statistics and by no means exhaustive. For a more detailed analysis, please refer to online resources or contact any of the workshop facilitators to discuss better. "],["installing-rrstudio.html", "Chapter 2 Installing R/RStudio 2.1 To download R 2.2 To download R Studio", " Chapter 2 Installing R/RStudio Installing R and RStudio, Installing and loading Packages Before you start programming in R, you will need to download and install R and RStudio on your computer. R is the programming language used for statistical analysis. R Studio is an Integrated Development Environment (IDE) and a Graphical User Interface (GUI) that allows for easy programming in R. It is important to note that you can use R without RStudio. However, you can not use RStudio without R. So, I encourage you to download both. 2.1 To download R R is hosted on the Comprehensive R Archive Network (CRAN) website (https://cran.r-project.org/). Once you are on the CRAN website, you would see three different links to download R. Please consider your computer operating system and select whichever best describes it. For example, if youre using a Windows computer, please click on R for windows. If this is your first time installing R and you have not used R before, please click on the 1st link you see (base- Install R for the first time); otherwise, update your R (if necessary) or download R Studio (see below for a guide on how to do this). If youre using Mac or Linus, follow the same procedure on the CRAN website to download and install R. Once you successfully install R, then you can download R studio. 2.2 To download R Studio To download R Studio, please visit this website (https://www.rstudio.com/products/rstudio/download/). Once you click on that link, download the R Studio version recommended for your computer and install it. Once you install R Studio, you can open it, and voila- welcome to the world of programming in R. That is all for all now. If you do not get everything now- dont worry, I will run through it again during the workshop. The aim of giving you this early is to fast-track the process. Feel free to ask questions about the installation on slack or during the workshop- I will be glad to answer. "],["data-structure-and-basic-progamming-in-rstudio.html", "Chapter 3 Data Structure and Basic Progamming in RStudio", " Chapter 3 Data Structure and Basic Progamming in RStudio "],["data-visualization.html", "Chapter 4 Data Visualization", " Chapter 4 Data Visualization "],["basic-statistical-analysis.html", "Chapter 5 Basic Statistical Analysis 5.1 Analysis of Variance (ANOVA)", " Chapter 5 Basic Statistical Analysis ANOVA, Correlation and Regression 5.1 Analysis of Variance (ANOVA) 5.1.1 One-Way ANOVA ANOVA is a parametric test and simply an extension of two-samples t-test. By Parametric, I mean it make assumptions regarding the shape of the population. Such assumption includes normal distribution in each factor level, commonly refers to as a bell-shaped curve, homogeneity (equal variance) and that the observations are independent. Basically, in your research or more broadly, statistics, you often hear or conduct one or two way ANOVA. What this means is about the factor in question (the number of predictors/explanatory/independent variables). In one-way ANOVA,you only have one independent (factor) variable and in a two-way ANOVA, you have two. We shall see examples below. When conducting ANOVA, you need to set up hypothesis. Basically, you have either H0 (null) or HA(Alternate hypothesis). Usually, your H0 hypothesis implies there is no difference in the mean of your groups. Simply put, your observations comes from populations with the same variance (homoscedastic). HA on the other hand states there is a difference (heteroscedastic). To test for this assumption of homoscedasticity, you can use the Levenes test (see below). N.B: if your H0 is rejected, you should not proceed with the standard ANOVA test- perhaps consider the equivalent non-parametric test (e.g., Kruskal-Wallis test) Ideally, you should state this out explicitly, such as below: #H0: there is no mean difference in the observation under consideration #HA: there is a significant difference. Enough of lecture, lets quickly demonstrate this with data We shall be using the Circadian data. So, lets load the data. For quick context, the data is about jet lag and adjusting to a different time zone. Campbell and Murphy (1998) claimed people adjust to their new time zone once the light reset their internal, circadian clock. Wright and Czeisler 2002 revisited this study and measured the circadian rhythm of melatonin production. They subjected 22 people to random treatments; control, knees only and eyes. Please read more on this paper online or attached pdf (Wright and Czeisler 2002). So for our analysis, we want to compare phase shifts in the circadian rhytm of melatonin productions in participants given another light treatments.  Load data Circadian &lt;- read.csv(file.choose()) # Load the csv named &quot;Circadian&quot; # Let&#39;s check what&#39;s in our data View(Circadian) # or use the &quot;dplyr&quot; package to randomly print 10 observations library(dplyr) dplyr::sample_n(Circadian, 10) ## treatment shift ## 1 eyes -1.52 ## 2 control -0.60 ## 3 knee -0.56 ## 4 knee -0.29 ## 5 control -1.27 ## 6 control -0.64 ## 7 eyes -1.35 ## 8 knee 0.03 ## 9 control -0.68 ## 10 knee -1.61 As you can see, this is a one-way factorial design. It has only one factor (the column treatment). Different treatments represents the levels in that factor and ideally are always ordered alphabetically. If you want to check the levels in your data, you can use the code below levels(Circadian$treatment) ## NULL # See it gives error right? You need to order them Circadian$treatment = ordered(Circadian$treatment, levels = c(&quot;control&quot;,&quot;eyes&quot;, &quot;knee&quot;)) # Check your levels now levels(Circadian$treatment) ## [1] &quot;control&quot; &quot;eyes&quot; &quot;knee&quot; Before we proceed with normal ANOVA calculation, lets play arond with codes and calculate the mean, median and standard deviation for this data. group_by(Circadian, treatment) %&gt;% summarise( count = n(), mean = mean(shift, na.rm = TRUE), sd = sd(shift, na.rm = TRUE) ) ## # A tibble: 3 x 4 ## treatment count mean sd ## &lt;ord&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 control 8 -0.309 0.618 ## 2 eyes 7 -1.55 0.706 ## 3 knee 7 -0.336 0.791 Also, you may want to even visualize this data and see how it looks. Lets install a package called ggpubr (which is from ggplot2 and used for publications mostly, hence the name ggpubr) if(!require(ggpubr)) {install.packages(&quot;ggpubr&quot;); library(ggpubr)} ggboxplot(Circadian, x = &quot;treatment&quot;, y = &quot;shift&quot;, color = &quot;treatment&quot;, palette = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), order = c(&quot;control&quot;, &quot;eyes&quot;, &quot;knee&quot;), ylab = &quot;Shift&quot;, xlab = &quot;Treatment&quot;) Now lets check our one-way ANOVA. ANOVA TEST Circadian_anova &lt;- lm( shift ~ treatment, data=Circadian) Circadian_anova ## ## Call: ## lm(formula = shift ~ treatment, data = Circadian) ## ## Coefficients: ## (Intercept) treatment.L treatment.Q ## -0.73196 -0.01907 1.00363 anova(Circadian_anova) ## Analysis of Variance Table ## ## Response: shift ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## treatment 2 7.2245 3.6122 7.2894 0.004472 ** ## Residuals 19 9.4153 0.4955 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Another method to calculate ANOVA using the aov function Circadian.aov &lt;- aov(shift ~ treatment, data = Circadian) summary(Circadian.aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## treatment 2 7.224 3.612 7.289 0.00447 ** ## Residuals 19 9.415 0.496 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Still the same as above, right? Good. It is important to check all assumptions. So, we need to check all assumptions of ANOVA here now. This is important to confirm the validity of our statistical tool. Test for homogeneity plot(Circadian_anova, 1) leveneTest(shift ~ factor(treatment), data=Circadian) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 2 0.1586 0.8545 ## 19 # If you run this directly, you should get error message (unless you have installed the package containing the &quot;levene&quot;) #install.packages(&quot;car&quot;, dependecies=TRUE) library(car) #Now run your levene test leveneTest(shift ~ factor(treatment), data=Circadian) # see that the P value is &gt; 0.05, there is no evidence to reject the H0 (they have the same variance) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 2 0.1586 0.8545 ## 19 Test for Normality plot(Circadian_anova, which=2) # This normality assumption can be further confirmed using the Shapiro normality test. circadian_residuals &lt;- residuals(object = Circadian_anova) shapiro.test(circadian_residuals) ## ## Shapiro-Wilk normality test ## ## data: circadian_residuals ## W = 0.95893, p-value = 0.468 # From the Shapiro normality test, the P-value = 0.468, which is greater than the chosen P-value (0.05) and therefore, we have strong evidence to conclude that this data comes from a normal population, as such normality assumption is met. Lastly, Since the ANOVA test is significant, you may want to compute the Tukey test to check for pairwise-comparison between the means of groups # Compute the Tukey result TukeyHSD(Circadian.aov) # Turkey will not accept anova generated from &quot;lm&quot; function. So use the aov here. ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = shift ~ treatment, data = Circadian) ## ## $treatment ## diff lwr upr p adj ## eyes-control -1.24267857 -2.1682364 -0.3171207 0.0078656 ## knee-control -0.02696429 -0.9525222 0.8985936 0.9969851 ## knee-eyes 1.21571429 0.2598022 2.1716263 0.0116776 # You can see that both eyes-control and knee-eyes are both significant (check the p-value) Also, maybe you try the Pairwise-test (not compulsory though) Pairwise t-test This can be used to compute pairwise comparisons between group levels with corrections for multiple testing. pairwise.t.test(Circadian$shift, Circadian$treatment, p.adjust.method = &quot;BH&quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: Circadian$shift and Circadian$treatment ## ## control eyes ## eyes 0.0066 - ## knee 0.9418 0.0066 ## ## P value adjustment method: BH # The BH means that- adjust the p-values by the Benjamini-Hochberg method. For question that came up in class (about printing your output to your report using broom package). Lets use this code. Assuming we want to print our Tukey result (of course, you can do that for any output), follow the code below: #install.packages(&#39;kableExtra&#39;) library(kableExtra) library(broom) # to tidy statistical models into data frames and for easy export Tukey_output = TukeyHSD(Circadian.aov) tidy(Tukey_output) # Tidy it ## # A tibble: 3 x 7 ## term contrast null.value estimate conf.low conf.high adj.p.value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 treatment eyes-control 0 -1.24 -2.17 -0.317 0.00787 ## 2 treatment knee-control 0 -0.0270 -0.953 0.899 0.997 ## 3 treatment knee-eyes 0 1.22 0.260 2.17 0.0117 Tidy_tukey = tidy(Tukey_output) # save it as data frame to &quot;kable&quot; it out kable(Tidy_tukey, &quot;html&quot;) %&gt;% kable_styling(&quot;striped&quot;, full_width = F) %&gt;% add_header_above() term contrast null.value estimate conf.low conf.high adj.p.value treatment eyes-control 0 -1.2426786 -2.1682364 -0.3171207 0.0078656 treatment knee-control 0 -0.0269643 -0.9525222 0.8985936 0.9969851 treatment knee-eyes 0 1.2157143 0.2598022 2.1716263 0.0116776 5.1.2 TWO-Way ANOVA Here, lets use fictional study, obviously, you could use your own data if you have. Fictional Study from (Hammed Akande). Context- Sclerophrys perreti is an endemic species to Nigeria, reportedly lost for over 50 years and rediscovered in 2013 at its type locality. It is considered to be a vulnerable species facing an extinction risk due to several threats, including predators, among other factors. The presence of predators in the habitats of Sclerophrys perreti poses a threat to the survival of this species. In this study, the aim is to check whether Sclerophrys perreti can survive in the presence or absence of predators and assess if this can be explained by the behavior of this predator species (day or night). This experiment involves monitoring the activities of Sclerophrys perreti in the habitat for three months during the rainy season. Hypothetically, we would expect that the survival of Sclerophrys perreti depends on the abundance of predators (presence or absence) in the neighboring environment, survey time (day or night) and to investigate if there is an interaction between the predictors (predator and survey time) on the abundance of Sclerophrys perreti Perret = read.csv(file.choose(), header = TRUE) # Load the csv file named &quot;AkandeData&quot; # ANOVA TEST Perret_anova &lt;- lm( Call ~ Predator*Survey, data=Perret) anova(Perret_anova) ## Analysis of Variance Table ## ## Response: Call ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Predator 1 17.6487 17.6487 14.8758 0.001395 ** ## Survey 1 3.1539 3.1539 2.6584 0.122528 ## Predator:Survey 1 5.8835 5.8835 4.9591 0.040664 * ## Residuals 16 18.9824 1.1864 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Test for homogeneity plot(Perret_anova, 1) leveneTest(Call ~ factor(Predator) * factor(Survey), data=Perret) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 3 1.687 0.2098 ## 16 # From the result of the &quot;levene test&quot;, given that the P-value is = 0.2098, which is greater than the chosen P-value of 0.05, we have strong evidence to accept the null hypothesis and conclude there&#39;s no evidence that all the treatment combination come from population with different variance. Therefore, the assumption of homogeneity is met. Assessing normality plot(Perret_anova, which=2) # This normality assumption can be further confirmed using the Shapiro normality test. anova_residuals &lt;- residuals(object = Perret_anova) shapiro.test(anova_residuals) ## ## Shapiro-Wilk normality test ## ## data: anova_residuals ## W = 0.95544, p-value = 0.4573 # As it can be observed from the QQ plot, most of the residuals falls on the straight line and implies that this data comes from a normally distributed population. This can be further confirmed using the Shapiro-Wilk normality test. From the Shapiro normality test, the P-value = 0.4573, which is greater than the chosen P-value (0.05) and therefore, we have strong evidence to conclude that this data comes from a normal population, as such normality assumption is met. # Interaction plot Perret_summary &lt;- Perret %&gt;% group_by(Survey, Predator) %&gt;% summarise(y_mean = mean(Call), y_se = psych::describe(Call)$se) ## `summarise()` has grouped output by &#39;Survey&#39;. You can override using the `.groups` argument. Perret_summary ## # A tibble: 4 x 4 ## # Groups: Survey [2] ## Survey Predator y_mean y_se ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 day &quot;Absence &quot; 4.35 0.683 ## 2 day &quot;Presence&quot; 3.3 0.256 ## 3 night &quot;Absence &quot; 6.54 1.04 ## 4 night &quot;Presence&quot; 3.22 0.212 Perret_summary%&gt;% ggplot(aes(x = Survey, y = y_mean, color = Predator)) + geom_line(aes(group = Predator)) + geom_point() + geom_errorbar(aes(ymin = y_mean-1.96*y_se, ymax = y_mean+1.96*y_se), width = .1) + labs(x = &quot;Survey Time&quot;,color = &quot;Predator&quot;, y = &quot;Activity Call (min)&quot;) + theme_classic() The end of ANOVA test. "],["introduction-to-species-distribution-modeling.html", "Chapter 6 Introduction to Species Distribution Modeling", " Chapter 6 Introduction to Species Distribution Modeling Hammed A. Akande This chapter briefly introduce you to Species Distribution Modeling (hereinafter, SDM) in R. In SDM, we relate the species occurrence data (e.g. in presence-absence format) with their environmental data (e.g. climatic data) to predict the probability of species occurring in an area or their habitat suitability. In this tutorial, I am assuming you have basic knowledge of Ecology/Wildlife/Conservation biology and statistics. I encourage you to watch my presentation video and read more online, especially if you dont have knowledge of ecology or ecological factors that can affect species distribution. Now, lets start the modeling exercise. To run this exercise, you need to install and load the required Packages. Again, I assume you know how to install and load packages, if not, refer to my Day 1 slide and video (or check the introductory section of this book). N.B- If you have installed the packages before, no need to install again, just load the library. #install.packages(&quot;dismo&quot;) #install.packages(&quot;maptools&quot;) #install.packages(&quot;maps&quot;) #install.packages(&quot;mapdata&quot;) #install.packages(&quot;dplyr&quot;) #install.packages(&quot;CoordinateCleaner&quot;) #install.packages(&quot;raster&quot;) #install.packages(&quot;ggplot2&quot;) #install.packages(&quot;scales&quot;) #install.packages(&quot;corrplot&quot;) library(dismo) library(maptools) library(maps) library(mapdata) library(dplyr) library(CoordinateCleaner) library(rgbif) library(corrplot) library(raster) Today, we are using the GBIF website to download species data (obviously, you can load in and use your own data if you have). We shall be using the Mona Monkey as study species. Again, please read about ecology of Mona Monkey and if you dont know of GBIF (I explained in class though) read more online about the organization. 6.0.1 Downloading the Species Data Mona &lt;- occ_search(scientificName = &quot;Cercopithecus mona&quot;, hasCoordinate=T) This function occ_search search for the species at the GBIF website and see if there are data available. If yes, the data will be downloaded and that is only the ones with coordinate. Remember I set hasCoordinate to be equal to TRUE (apparently, you want to spatially model data with coordinates) The output will be stored as Mona (in form of list), but we only need the data part of it, so retain the data alone. Mona = Mona$data View(Mona) #you can view the species to see how its structured head(Mona) # to see the first 10 observations ## # A tibble: 6 x 142 ## key scientificName decimalLatitude decimalLongitude issues datasetKey publishingOrgKey installationKey ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 25739~ Cercopithecus ~ 12.1 -61.7 gass84 50c9509d-~ 28eb1a3f-1c15-4~ 997448a8-f762-~ ## 2 28234~ Cercopithecus ~ 12.1 -61.7 cdroun~ 50c9509d-~ 28eb1a3f-1c15-4~ 997448a8-f762-~ ## 3 33113~ Cercopithecus ~ 7.04 2.00 cdroun~ 70d19075-~ 071d63b5-ae0b-4~ 842b6a28-624a-~ ## 4 24451~ Cercopithecus ~ 0.0459 6.53 cdroun~ 50c9509d-~ 28eb1a3f-1c15-4~ 997448a8-f762-~ ## 5 26112~ Cercopithecus ~ 6.96 2.10 cdroun~ 50c9509d-~ 28eb1a3f-1c15-4~ 997448a8-f762-~ ## 6 29885~ Cercopithecus ~ 12.1 -61.7 cdroun~ 50c9509d-~ 28eb1a3f-1c15-4~ 997448a8-f762-~ ## # ... with 134 more variables: publishingCountry &lt;chr&gt;, protocol &lt;chr&gt;, lastCrawled &lt;chr&gt;, lastParsed &lt;chr&gt;, ## # crawlId &lt;int&gt;, hostingOrganizationKey &lt;chr&gt;, basisOfRecord &lt;chr&gt;, occurrenceStatus &lt;chr&gt;, ## # taxonKey &lt;int&gt;, kingdomKey &lt;int&gt;, phylumKey &lt;int&gt;, classKey &lt;int&gt;, orderKey &lt;int&gt;, familyKey &lt;int&gt;, ## # genusKey &lt;int&gt;, speciesKey &lt;int&gt;, acceptedTaxonKey &lt;int&gt;, acceptedScientificName &lt;chr&gt;, kingdom &lt;chr&gt;, ## # phylum &lt;chr&gt;, order &lt;chr&gt;, family &lt;chr&gt;, genus &lt;chr&gt;, species &lt;chr&gt;, genericName &lt;chr&gt;, ## # specificEpithet &lt;chr&gt;, taxonRank &lt;chr&gt;, taxonomicStatus &lt;chr&gt;, iucnRedListCategory &lt;chr&gt;, ## # dateIdentified &lt;chr&gt;, stateProvince &lt;chr&gt;, year &lt;int&gt;, month &lt;int&gt;, day &lt;int&gt;, eventDate &lt;chr&gt;, ## # modified &lt;chr&gt;, lastInterpreted &lt;chr&gt;, references &lt;chr&gt;, license &lt;chr&gt;, identifiers &lt;chr&gt;, facts &lt;chr&gt;, ## # relations &lt;chr&gt;, isInCluster &lt;lgl&gt;, geodeticDatum &lt;chr&gt;, class &lt;chr&gt;, countryCode &lt;chr&gt;, ## # recordedByIDs &lt;chr&gt;, identifiedByIDs &lt;chr&gt;, country &lt;chr&gt;, rightsHolder &lt;chr&gt;, identifier &lt;chr&gt;, ## # http...unknown.org.nick &lt;chr&gt;, verbatimEventDate &lt;chr&gt;, datasetName &lt;chr&gt;, verbatimLocality &lt;chr&gt;, ## # gbifID &lt;chr&gt;, collectionCode &lt;chr&gt;, occurrenceID &lt;chr&gt;, taxonID &lt;chr&gt;, catalogNumber &lt;chr&gt;, ## # recordedBy &lt;chr&gt;, http...unknown.org.occurrenceDetails &lt;chr&gt;, institutionCode &lt;chr&gt;, rights &lt;chr&gt;, ## # eventTime &lt;chr&gt;, identifiedBy &lt;chr&gt;, identificationID &lt;chr&gt;, name &lt;chr&gt;, ## # coordinateUncertaintyInMeters &lt;dbl&gt;, projectId &lt;chr&gt;, continent &lt;chr&gt;, habitat &lt;chr&gt;, locality &lt;chr&gt;, ## # samplingProtocol &lt;chr&gt;, occurrenceRemarks &lt;chr&gt;, individualCount &lt;int&gt;, lifeStage &lt;chr&gt;, ## # vernacularName &lt;chr&gt;, higherClassification &lt;chr&gt;, collectionID &lt;chr&gt;, programmeAcronym &lt;chr&gt;, ## # organismQuantityType &lt;chr&gt;, institutionID &lt;chr&gt;, county &lt;chr&gt;, ## # http...rs.tdwg.org.dwc.terms.organismQuantityType &lt;chr&gt;, collectionKey &lt;chr&gt;, institutionKey &lt;chr&gt;, ## # identifiedByIDs.type &lt;chr&gt;, identifiedByIDs.value &lt;chr&gt;, sex &lt;chr&gt;, ## # X.99d66b6c.9087.452f.a9d4.f15f2c2d0e7e. &lt;chr&gt;, establishmentMeans &lt;chr&gt;, higherGeography &lt;chr&gt;, ## # nomenclaturalCode &lt;chr&gt;, endDayOfYear &lt;chr&gt;, georeferenceVerificationStatus &lt;chr&gt;, language &lt;chr&gt;, ## # type &lt;chr&gt;, startDayOfYear &lt;chr&gt;, accessRights &lt;chr&gt;, ... How about we define the extent of our species to know the min and max longitude and latitude of the species? That should make sense I guess. So, set the geographic extent. max.lat &lt;- ceiling(max(Mona$decimalLatitude)) min.lat &lt;- floor(min(Mona$decimalLatitude)) max.lon &lt;- ceiling(max(Mona$decimalLongitude)) min.lon &lt;- floor(min(Mona$decimalLongitude)) geographic.extent &lt;- extent(x = c(min.lon, max.lon, min.lat, max.lat)) geographic.extent ## class : Extent ## xmin : -62 ## xmax : 30 ## ymin : -5 ## ymax : 49 Now, lets just check it on map to even know where our species are located in space. data(wrld_simpl) # Base map plot(wrld_simpl, xlim = c(min.lon, max.lon), ylim = c(min.lat, max.lat), axes = TRUE, col = &quot;grey95&quot;) # Individual obs points points(x = Mona$decimalLongitude, y = Mona$decimalLatitude, col = &quot;red&quot;, pch = 20, cex = 0.75) box() Voila! Thats better. Looking at something in picture/plot/map make more sense I guess. 6.0.2 Cleaning the coordinates and checking for outliers At least now you know where they are located, but are you really sure all points are accurate? Do they all look correct? Do you think there might be errors in species collection or even when recording them? Or might be biased in any way? So, the best way to be sure is to do some Data quality checking. Lets use a package called CoordinateCleaner for this. I am testing for duplicates, centroids outliers and to check how far away from biodiversity institutions. use the code ?CoordinateCleaner to know more about what you can test for. clean_Mona &lt;- clean_coordinates(Mona, lon=&quot;decimalLongitude&quot;,lat=&quot;decimalLatitude&quot;, tests=c(&quot;centroids&quot;, &quot;outliers&quot;, &quot;duplicates&quot;, &quot;institutions&quot;),inst_rad = 10000) ## Testing coordinate validity ## Flagged 0 records. ## Testing country centroids ## Flagged 1 records. ## Testing geographic outliers ## Flagged 23 records. ## Testing biodiversity institutions ## Flagged 14 records. ## Testing duplicates ## Flagged 185 records. ## Flagged 206 of 373 records, EQ = 0.55. Wow, 206 of 373 flagged. See why you need to clean data now? Else, your model(s) will be biased. Lets subset the data to only cleaned version now. clean_Mona = clean_Mona[clean_Mona$.summary,] If you check the clean_Mona, you will see there are many variables. We really dont need all of them for analysis, so why not just retain the variables we only need clean_Mona = clean_Mona[, c(&quot;species&quot;, &quot;decimalLatitude&quot;, &quot;decimalLongitude&quot;)] # or through the dplyr package. Of course, you will get the same result clean_Mona &lt;-clean_Mona %&gt;% dplyr::select(species, decimalLatitude, decimalLongitude) Remember the data contains just the name of the species. We need it in presence/absence format (I explained in class). So, lets turn the species name to 1 (for presence) Mona_P &lt;- data.frame(clean_Mona, occ=1) head(Mona_P) ## species decimalLatitude decimalLongitude occ ## 3 Cercopithecus mona 7.038812 1.999688 1 ## 4 Cercopithecus mona 0.045902 6.528708 1 ## 5 Cercopithecus mona 6.956431 2.097323 1 ## 7 Cercopithecus mona 7.721062 -1.701965 1 ## 8 Cercopithecus mona 6.438185 3.534619 1 ## 9 Cercopithecus mona 6.591946 3.948594 1 # You see a new column is now added, called &quot;occ&quot; (as in short form of occurence) # If you wish to export this clean data (e.g. to csv), for further analysis, you can do that now. #write.csv(clean_Mona, &quot;Mona_cleaned.csv&quot;, row.names = FALSE) Dont forget that SDM (as in the case of correlative), we want to relate the species with their environment to understand factors affecting them. So, lets the Get the climate data. 6.0.3 Download Climate data You can get climate data from worldclim, chelsa and paleoclim, among others. # You may want to set directory to store it if(!dir.exists(&quot;bioclim_data&quot;)){ dir.create(&quot;bioclim_data&quot;, recursive = TRUE) } clim_data &lt;- getData(name = &quot;worldclim&quot;, var = &quot;bio&quot;, res = 5, path = &quot;bioclim_data&quot;, download = T) In SDM, to every presence, there should be absence. As you may know, absence data are often not available and so we can generate background (or pseudo-absence) data. 6.0.4 Generate Background data Lets generate Background data using the climate data we just downloaded as the sampling resolution bil.files &lt;- list.files(path = &quot;bioclim_data/wc5&quot;, pattern = &quot;*.bil$&quot;, full.names = TRUE) # Let&#39;s just use one of the .bil files to mask the background data, we don&#39;t really need all mask &lt;- raster(bil.files[1]) # Use the randomPoints function to randomly sample points. Now, we shall sample the same number of points as our observed points (and extend it by 1.25). By sampling same number of occurence point and giving a bit room for extension, we are conservative enough and reduce bias. background &lt;- randomPoints(mask = mask, n = nrow(Mona_P), ext = geographic.extent, extf = 1.25) How about we Plot them on map (presence and pseudo-absence) plot(wrld_simpl, xlim = c(min.lon, max.lon), ylim = c(min.lat, max.lat), axes = TRUE, col = &quot;grey35&quot;, main = &quot;Presence and pseudo-absence points&quot;) # Add the background points points(background, col = &quot;green&quot;, pch = 1, cex = 0.75) # Add the observations points(x = Mona_P$decimalLongitude, y = Mona_P$decimalLatitude, col = &quot;red&quot;, pch = 20, cex = 0.75) box() Now, what we can do is to join them together. Mona_P = Mona_P[, c(&quot;decimalLongitude&quot;, &quot;decimalLatitude&quot;, &quot;occ&quot;)] # since we don&#39;t need the column &quot;species&quot; again, we can remove it. background_dat &lt;- data.frame(background) # put it in dataframe summary(background_dat) ## x y ## Min. :-72.792 Min. :-11.542 ## 1st Qu.: -2.708 1st Qu.: 1.458 ## Median : 12.958 Median : 14.792 ## Mean : 4.572 Mean : 17.108 ## 3rd Qu.: 29.167 3rd Qu.: 29.792 ## Max. : 40.625 Max. : 55.042 names(background_dat) &lt;- c(&#39;decimalLongitude&#39;,&#39;decimalLatitude&#39;) # set the name of background_dat instead form &quot;x&quot; and &quot;y&quot; to Longitude and Latitude background_dat$occ &lt;- 0 # set absence data to 0 (remember we set presence to 1) summary(background_dat) ## decimalLongitude decimalLatitude occ ## Min. :-72.792 Min. :-11.542 Min. :0 ## 1st Qu.: -2.708 1st Qu.: 1.458 1st Qu.:0 ## Median : 12.958 Median : 14.792 Median :0 ## Mean : 4.572 Mean : 17.108 Mean :0 ## 3rd Qu.: 29.167 3rd Qu.: 29.792 3rd Qu.:0 ## Max. : 40.625 Max. : 55.042 Max. :0 Mona_PA &lt;- rbind(Mona_P, background_dat) # use the &quot;rbind&quot; function to row bind them. summary(Mona_PA) ## decimalLongitude decimalLatitude occ ## Min. :-72.792 Min. :-11.542 Min. :0.0 ## 1st Qu.: 1.738 1st Qu.: 5.896 1st Qu.:0.0 ## Median : 2.686 Median : 6.909 Median :0.5 ## Mean : 3.857 Mean : 11.830 Mean :0.5 ## 3rd Qu.: 14.417 3rd Qu.: 14.792 3rd Qu.:1.0 ## Max. : 40.625 Max. : 55.042 Max. :1.0 Mona_PA = data.frame(Mona_PA) dplyr::sample_n(Mona_PA, 10) # randomly check 10 observations ## decimalLongitude decimalLatitude occ ## 92 2.512721 6.904417 1 ## 240 9.400000 5.833300 1 ## 321 -65.875000 -7.541667 0 ## 110 2.427500 8.490000 1 ## 541 -68.291667 8.291667 0 ## 179 -13.125000 11.625000 0 ## 259 18.000000 9.000000 1 ## 125 2.166667 6.916667 1 ## 751 14.041667 28.625000 0 ## 1410 38.375000 14.625000 0 6.0.5 Extract the environmental data for the Mona coordinate Mona_PA = cbind(Mona_PA, raster::extract(x = clim_data, y = data.frame(Mona_PA[,c(&#39;decimalLongitude&#39;,&#39;decimalLatitude&#39;)]), cellnumbers=T )) # Check if there are duplicated cells duplicated(Mona_PA$cells) ## [1] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE TRUE ## [18] FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [35] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE TRUE ## [52] TRUE FALSE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE TRUE ## [69] TRUE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE ## [86] TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE FALSE TRUE TRUE TRUE ## [103] TRUE TRUE FALSE TRUE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [120] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE ## [137] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE ## [154] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [171] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [188] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [205] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [222] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [239] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [256] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [273] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [290] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [307] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [324] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE You can see some duplicated cells, right? So, lets retain non-duplicated cells (obviously, you dont want to have duplicated cells in analysis) Retain non-duplicated cells Mona_PA &lt;- Mona_PA[!duplicated(Mona_PA$cells),] # Now check again if there are duplicated cells (I am certain it will all be FALSE now) duplicated(Mona_PA$cells) ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [18] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [35] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [52] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [69] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [86] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [103] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [120] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [137] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [154] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [171] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [188] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [205] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [222] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [239] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [256] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [273] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [290] FALSE Check for missing values (NA) any(is.na(Mona_PA)) # Check for NA ## [1] TRUE Clear enough right? We have missing values, so lets remove them Remove NA Mona_PA = na.omit(Mona_PA) # remove NA # check again. This time, it should be FALSE any(is.na(Mona_PA)) ## [1] FALSE Thats it. We can start the process of model fitting Before we even start, its a good idea to test for multicollinearity (to be sure we dont have multicollinear variables). I explained why this is not good in class- watch the video or read more online. 6.0.6 Test for Multicollinearity Build a correlation matrix cor_mat &lt;- cor(Mona_PA[,-c(1:6)], method=&#39;spearman&#39;) corrplot.mixed(cor_mat, tl.pos=&#39;d&#39;, tl.cex=0.6, number.cex=0.5, addCoefasPercent=T) We can use a function called select07 to remove highly correlated variables (variables greater than 70% = 0.7). (See Dorman et al 2013) library(devtools) #devtools::install_git(&quot;https://gitup.uni-potsdam.de/macroecology/mecofun.git&quot;) library(mecofun) # Run select07() var_sel &lt;- select07(X=Mona_PA[,-c(1:4)], y=Mona_PA$occ, threshold=0.7) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred # Check out the structure of the resulting object: str(var_sel) ## List of 3 ## $ AIC : Named num [1:19] 197 197 213 241 257 ... ## ..- attr(*, &quot;names&quot;)= chr [1:19] &quot;bio4&quot; &quot;bio6&quot; &quot;bio3&quot; &quot;bio7&quot; ... ## $ cor_mat : num [1:19, 1:19] 1 -0.108 0.365 -0.277 0.494 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:19] &quot;bio1&quot; &quot;bio2&quot; &quot;bio3&quot; &quot;bio4&quot; ... ## .. ..$ : chr [1:19] &quot;bio1&quot; &quot;bio2&quot; &quot;bio3&quot; &quot;bio4&quot; ... ## $ pred_sel: chr [1:9] &quot;bio4&quot; &quot;bio6&quot; &quot;bio15&quot; &quot;bio19&quot; ... # Extract the names of the weakly correlated predictors in order of their AIC: pred_sel = var_sel$pred_sel pred_sel ## [1] &quot;bio4&quot; &quot;bio6&quot; &quot;bio15&quot; &quot;bio19&quot; &quot;bio9&quot; &quot;bio10&quot; &quot;bio2&quot; &quot;bio8&quot; &quot;bio18&quot; See important variables in that order 6.0.7 Model selection We can fit different regression model to predict our species. This model can take linear function, quadratic or polynomial. We can then use vif or AIC to determine which one work best for this model. For the sake of this exercise, I will only fit Linear relationship. # Take any bioclim variable and fit a GLM assuming a linear relationship: model_linear &lt;- glm(occ ~ bio19, family=binomial(link=logit), data= Mona_PA) summary(model_linear) ## ## Call: ## glm(formula = occ ~ bio19, family = binomial(link = logit), data = Mona_PA) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.0272 -0.7494 -0.6247 1.0343 1.6464 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.5350688 0.2064067 -7.437 1.03e-13 *** ## bio19 0.0038553 0.0005399 7.141 9.26e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 393.60 on 288 degrees of freedom ## Residual deviance: 310.23 on 287 degrees of freedom ## AIC: 314.23 ## ## Number of Fisher Scoring iterations: 4 Okay, lets fit a quadratic relationship with the same bioclim var used above: model_quad &lt;- glm(occ ~ bio19 + I(bio19^2), family=binomial(link=logit), data= Mona_PA) summary(model_quad) ## ## Call: ## glm(formula = occ ~ bio19 + I(bio19^2), family = binomial(link = logit), ## data = Mona_PA) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1023 -0.6376 -0.3866 0.7762 1.7656 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.557e+00 3.115e-01 -8.207 2.27e-16 *** ## bio19 1.071e-02 1.363e-03 7.860 3.83e-15 *** ## I(bio19^2) -6.134e-06 1.003e-06 -6.112 9.81e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 393.60 on 288 degrees of freedom ## Residual deviance: 271.78 on 286 degrees of freedom ## AIC: 277.78 ## ## Number of Fisher Scoring iterations: 4 We can now use a Maximum likelihood estimator to select which model is best and fit the SDM. N.B- the lower your AIC, the better. So any model with lower AIC value is the best model to be selected. AIC(model_linear) ## [1] 314.2324 AIC(model_quad) ## [1] 277.7795 Voila! Ideally, including the interaction term (quadratic function) seems to make more sense for this model. However, as I said earlier, for the sake of this exercise, I will just continue with linear model to demonstrate what we really want to know. If you want to do more (include quadratic or anything), you can go ahead using the same model formula above or reach out to me if you have issues or concerns. 6.0.8 Fitting the model Now that we know which model to fit, we can select the model and then evaluate the prediction. # regression model model = step(glm(occ ~ bio4 + bio6 + bio15 + bio19, family=binomial(link=logit), data= Mona_PA)) ## Start: AIC=183.44 ## occ ~ bio4 + bio6 + bio15 + bio19 ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Df Deviance AIC ## - bio15 1 173.53 181.53 ## &lt;none&gt; 173.44 183.44 ## - bio4 1 179.20 187.20 ## - bio19 1 187.58 195.58 ## - bio6 1 263.93 271.93 ## ## Step: AIC=181.53 ## occ ~ bio4 + bio6 + bio19 ## ## Df Deviance AIC ## &lt;none&gt; 173.53 181.53 ## - bio4 1 181.99 187.99 ## - bio19 1 187.62 193.62 ## - bio6 1 265.54 271.54 summary(model) ## ## Call: ## glm(formula = occ ~ bio4 + bio6 + bio19, family = binomial(link = logit), ## data = Mona_PA) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.10852 -0.30152 -0.00401 0.49153 1.94663 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.584e+01 2.342e+00 -6.762 1.36e-11 *** ## bio4 8.983e-04 2.862e-04 3.139 0.001697 ** ## bio6 7.347e-02 1.034e-02 7.105 1.20e-12 *** ## bio19 2.205e-03 6.460e-04 3.413 0.000642 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 393.60 on 288 degrees of freedom ## Residual deviance: 173.53 on 285 degrees of freedom ## AIC: 181.53 ## ## Number of Fisher Scoring iterations: 7 #Let&#39;s see the plot of Occurrence my_preds &lt;- c(&#39;bio4&#39;, &#39;bio6&#39;, &quot;bio15&quot;, &quot;bio19&quot;) bio_clim_df1 &lt;- data.frame(rasterToPoints(clim_data[[my_preds]])) any(is.na(bio_clim_df1)) ## [1] TRUE bio_clim_df1&lt;- na.omit(bio_clim_df1) Model_glm_pred &lt;- rasterFromXYZ(cbind(bio_clim_df1[,1:2],predict(model, bio_clim_df1, type=&#39;response&#39;))) plot((Model_glm_pred), xlim = c(min(Mona_PA$decimalLongitude),max (Mona_PA$decimalLongitude)), ylim = c(min(Mona_PA$decimalLatitude), max(Mona_PA$decimalLatitude)), main=&#39;Probability of Occurence&#39;, axes=F) Good. You can see the habitat suitability right? or the probability of occurrence for Mona Monkey. How about we zoom in to Africa and check it well? Run the code below to zoom into Africa plot((Model_glm_pred), xlim = c(min(-25),max (50)), ylim = c(min(-40), max(40)), main=&#39;Probability of Occurence- Mona Monkey&#39;, axes=F) You may want to assess the goodness of fit # Explained deviance: expl_deviance(obs = Mona_PA$occ, pred = model$fitted) ## [1] 0.5591346 55.9% of the predictors explained the deviance in the model Okay, thats not what we want to do with SDM here. Lets transfer the probability of occurence to binary prediction 6.0.9 Model evaluation and validation Because we need to evaluate the prediction (of course if you write exam, you want to know how well you perform), so we need to set up evaluation dataset. The approach to do this (as in remote sensing) is to divide (randomly) the data into testing and training. So, lets set out 70% of our Mona monkey as training data and the remaining 30% for testing later. Lastly, we have selected linear function up there, so we are good to go and can fit different algorithms now. Split and train the model # Use 70% for training data (of course you can change it and use 60 or 80% depending on you) train_data &lt;- sample(seq_len(nrow(Mona_PA)), size=round(0.7*nrow(Mona_PA))) # Okay, let&#39;s subset the training &amp; testing data Mona_train &lt;- Mona_PA[train_data,] Mona_test &lt;- Mona_PA[-train_data,] # If you want to store the split information for later use, use this code: #write(train_data, file = &quot;Mona_traindata.txt&quot;) #remember I said we can store other file than csv alone right?) Using our GLM regression (but now on the training data) to evaluate how well it perform model_glm = step(glm(occ ~ bio4 + bio6 + bio15 + bio19, family=binomial(link=logit), data= Mona_train)) ## Start: AIC=133.16 ## occ ~ bio4 + bio6 + bio15 + bio19 ## ## Df Deviance AIC ## - bio15 1 123.17 131.17 ## &lt;none&gt; 123.16 133.16 ## - bio4 1 128.90 136.90 ## - bio19 1 130.47 138.47 ## - bio6 1 184.25 192.25 ## ## Step: AIC=131.17 ## occ ~ bio4 + bio6 + bio19 ## ## Df Deviance AIC ## &lt;none&gt; 123.17 131.17 ## - bio4 1 129.74 135.74 ## - bio19 1 130.60 136.60 ## - bio6 1 185.95 191.95 summary(model_glm) ## ## Call: ## glm(formula = occ ~ bio4 + bio6 + bio19, family = binomial(link = logit), ## data = Mona_train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.95547 -0.29929 -0.00348 0.48366 2.02285 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.574e+01 2.877e+00 -5.471 4.49e-08 *** ## bio4 9.193e-04 3.352e-04 2.742 0.0061 ** ## bio6 7.370e-02 1.284e-02 5.740 9.48e-09 *** ## bio19 1.799e-03 7.148e-04 2.517 0.0118 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 277.17 on 201 degrees of freedom ## Residual deviance: 123.17 on 198 degrees of freedom ## AIC: 131.17 ## ## Number of Fisher Scoring iterations: 7 You may want to check the response curve my_preds = c(&quot;bio4&quot;, &quot;bio6&quot;, &quot;bio15&quot;, &quot;bio19&quot;) preds_cv &lt;- crossvalSDM(model_glm, traindat = Mona_train, colname_species = &#39;occ&#39;, colname_pred = my_preds) plot(model_glm$fitted.values, preds_cv, xlab=&#39;Fitted values&#39;, ylab=&#39;Predicted values from CV&#39;) abline(0,1,col=&#39;red&#39;,lwd=2) Before we map the prediction, lets threshold the data (and try check the threshold independent metrics- AUC) Thresholding library(PresenceAbsence) # Cross-validated predictions: threshold_data &lt;- data.frame(ID = seq_len(nrow(Mona_train)), obs = Mona_train$occ, pred = preds_cv) # Get the optimal thresholds: (threshold_optimal &lt;- PresenceAbsence::optimal.thresholds(DATA= threshold_data)) ## Warning in PresenceAbsence::optimal.thresholds(DATA = threshold_data): req.sens defaults to 0.85 ## Warning in PresenceAbsence::optimal.thresholds(DATA = threshold_data): req.spec defaults to 0.85 ## Warning in PresenceAbsence::optimal.thresholds(DATA = threshold_data): costs assumed to be equal ## Method pred ## 1 Default 0.5000000 ## 2 Sens=Spec 0.4850000 ## 3 MaxSens+Spec 0.3700000 ## 4 MaxKappa 0.3700000 ## 5 MaxPCC 0.3700000 ## 6 PredPrev=Obs 0.5350000 ## 7 ObsPrev 0.4405941 ## 8 MeanProb 0.4402461 ## 9 MinROCdist 0.3700000 ## 10 ReqSens 0.5000000 ## 11 ReqSpec 0.4800000 ## 12 Cost 0.3700000 Good. You can now use any values above to threshold your species data to presence and absence # Threshold using the max sen+spec # Print the confusion Matrix (cmx_maxSSS &lt;- PresenceAbsence::cmx(DATA= threshold_data, threshold=threshold_optimal[3,2])) ## observed ## predicted 1 0 ## 1 85 18 ## 0 4 95 Lets compute AUC library(AUC) # Let&#39;s have a look a the ROC curve: roc_cv &lt;- roc(preds_cv, as.factor(Mona_train$occ)) plot(roc_cv, col = &quot;grey70&quot;, lwd = 2) Compute the AUC and other evaluation metrics: (evaluation_metrics = evalSDM(Mona_train$occ, preds_cv, thresh.method = &quot;MaxSens+Spec&quot;)) ## AUC TSS Kappa Sens Spec PCC D2 thresh ## 1 0.9281098 0.7957641 0.7826895 0.9550562 0.840708 0.8910891 0.5246774 0.37 We can now validate the model performance on the test data (performance_glm &lt;- evalSDM(Mona_test$occ, predict(model_glm, Mona_test[,my_preds], type=&#39;response&#39;), thresh.method = &quot;MaxSens+Spec&quot;)) ## AUC TSS Kappa Sens Spec PCC D2 thresh ## 1 0.9393939 0.7441077 0.7529813 0.8181818 0.9259259 0.8850575 0.5591624 0.49 Please note- Sensitivity = true positive rate Specificity = true negative rate PCC = Proportion of correctly classified observations, We can evaluate if the model is good or not with TSS (true skill statistics or Kappa). You can also chekc AUC (Area under the curve). You may ask which curve, the ROC curve- Receiver operating characteristics. 6.0.10 Map prediction Now, lets check the Map prediction by plotting the main binary map with the data- bio_clim_df_2 &lt;- data.frame(rasterToPoints(clim_data[[my_preds]])) any(is.na(bio_clim_df_2)) ## [1] TRUE bio_clim_df_2&lt;- na.omit(bio_clim_df_2) binary_glm &lt;- predicted_glm &lt;- rasterFromXYZ(cbind(bio_clim_df_2[,1:2],predict(model_glm, bio_clim_df_2, type=&#39;response&#39;))) values(binary_glm) &lt;- ifelse(values(predicted_glm)&gt;= performance_glm$thresh, 1, 0) plot(stack(predicted_glm, binary_glm), xlim = c(min(-25),max (50)), ylim = c(min(-40), max(40)), main=c(&#39;Probability of Occurrence-Mona&#39;,&#39;Binary Prediction-Mona&#39;), axes=F) Now, you can see the binary prediction of Mona Monkey throughout Africa. Great! We stopped here in class. I will update the book later as time permits (check back soon): Transfer this prediction to future (2050 or 2070) Use different model algorithms (random forest, boosted regression trees, etc) Ensemble the models (to account for model uncertainty) and lots more. If you have questions, feel free to ask email me or slack me. "],["resources-and-references.html", "Chapter 7 Resources and References", " Chapter 7 Resources and References "]]
